{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GCN-Cora.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYNZ3Pwgbanz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b5067e01-8277-4f22-b5b7-cbf6d2f0adce"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuuyGcE8bo7s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "2679e1db-5bf3-46aa-fb6e-48d0990510b0"
      },
      "source": [
        "!git clone https://github.com/tkipf/pygcn.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pygcn'...\n",
            "remote: Enumerating objects: 78, done.\u001b[K\n",
            "remote: Total 78 (delta 0), reused 0 (delta 0), pack-reused 78\u001b[K\n",
            "Unpacking objects: 100% (78/78), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-upaHjHcMou",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5613938b-5644-48a3-a969-7c2d1b3bd45c"
      },
      "source": [
        "%cd pygcn/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Python/GitHub/pygcn/pygcn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFaT9epTcUVF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5cc4b768-2791-46f4-8754-404134188df6"
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from pygcn.utils import load_data, accuracy\n",
        "from pygcn.models import GCN\n",
        "\n",
        "# Training settings\n",
        "\n",
        "no_cuda = False\n",
        "fastmode = False\n",
        "seed = 42\n",
        "epochs = 200\n",
        "lr = .01\n",
        "weight_decay = 5e-4\n",
        "hidden = 16\n",
        "dropout = 0.5\n",
        "\n",
        "cuda = not no_cuda and torch.cuda.is_available()\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(seed)\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "# Load data\n",
        "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
        "\n",
        "# Model and optimizer\n",
        "model = GCN(nfeat=features.shape[1],\n",
        "            nhid=hidden,\n",
        "            nclass=labels.max().item() + 1,\n",
        "            dropout=dropout)\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "    features = features.cuda()\n",
        "    adj = adj.cuda()\n",
        "    labels = labels.cuda()\n",
        "    idx_train = idx_train.cuda()\n",
        "    idx_val = idx_val.cuda()\n",
        "    idx_test = idx_test.cuda()\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(features, adj)\n",
        "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if not fastmode:\n",
        "        # Evaluate validation set performance separately,\n",
        "        # deactivates dropout during validation run.\n",
        "        model.eval()\n",
        "        output = model(features, adj)\n",
        "\n",
        "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
        "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "    print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "\n",
        "\n",
        "# Train model\n",
        "t_total = time.time()\n",
        "for epoch in range(epochs):\n",
        "    train(epoch)\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "# Testing\n",
        "test()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading cora dataset...\n",
            "Epoch: 0001 loss_train: 1.9607 acc_train: 0.1143 loss_val: 1.9531 acc_val: 0.1133 time: 0.1731s\n",
            "Epoch: 0002 loss_train: 1.9564 acc_train: 0.1214 loss_val: 1.9404 acc_val: 0.1133 time: 0.0059s\n",
            "Epoch: 0003 loss_train: 1.9409 acc_train: 0.1357 loss_val: 1.9282 acc_val: 0.1133 time: 0.0052s\n",
            "Epoch: 0004 loss_train: 1.9155 acc_train: 0.2214 loss_val: 1.9163 acc_val: 0.1300 time: 0.0051s\n",
            "Epoch: 0005 loss_train: 1.9015 acc_train: 0.2500 loss_val: 1.9047 acc_val: 0.3567 time: 0.0051s\n",
            "Epoch: 0006 loss_train: 1.9060 acc_train: 0.2357 loss_val: 1.8935 acc_val: 0.3500 time: 0.0053s\n",
            "Epoch: 0007 loss_train: 1.8993 acc_train: 0.2857 loss_val: 1.8829 acc_val: 0.3500 time: 0.0055s\n",
            "Epoch: 0008 loss_train: 1.8803 acc_train: 0.2929 loss_val: 1.8725 acc_val: 0.3500 time: 0.0054s\n",
            "Epoch: 0009 loss_train: 1.8770 acc_train: 0.2929 loss_val: 1.8621 acc_val: 0.3500 time: 0.0050s\n",
            "Epoch: 0010 loss_train: 1.8638 acc_train: 0.3214 loss_val: 1.8518 acc_val: 0.3500 time: 0.0049s\n",
            "Epoch: 0011 loss_train: 1.8565 acc_train: 0.3000 loss_val: 1.8415 acc_val: 0.3500 time: 0.0052s\n",
            "Epoch: 0012 loss_train: 1.8437 acc_train: 0.3000 loss_val: 1.8312 acc_val: 0.3500 time: 0.0055s\n",
            "Epoch: 0013 loss_train: 1.8384 acc_train: 0.3143 loss_val: 1.8212 acc_val: 0.3500 time: 0.0051s\n",
            "Epoch: 0014 loss_train: 1.8144 acc_train: 0.3143 loss_val: 1.8115 acc_val: 0.3500 time: 0.0052s\n",
            "Epoch: 0015 loss_train: 1.8133 acc_train: 0.3071 loss_val: 1.8020 acc_val: 0.3500 time: 0.0054s\n",
            "Epoch: 0016 loss_train: 1.7865 acc_train: 0.3143 loss_val: 1.7929 acc_val: 0.3500 time: 0.0053s\n",
            "Epoch: 0017 loss_train: 1.7732 acc_train: 0.3071 loss_val: 1.7841 acc_val: 0.3500 time: 0.0055s\n",
            "Epoch: 0018 loss_train: 1.7714 acc_train: 0.3214 loss_val: 1.7757 acc_val: 0.3500 time: 0.0051s\n",
            "Epoch: 0019 loss_train: 1.7587 acc_train: 0.2929 loss_val: 1.7676 acc_val: 0.3500 time: 0.0050s\n",
            "Epoch: 0020 loss_train: 1.7644 acc_train: 0.3071 loss_val: 1.7602 acc_val: 0.3500 time: 0.0052s\n",
            "Epoch: 0021 loss_train: 1.7852 acc_train: 0.3071 loss_val: 1.7534 acc_val: 0.3500 time: 0.0051s\n",
            "Epoch: 0022 loss_train: 1.7599 acc_train: 0.3286 loss_val: 1.7471 acc_val: 0.3500 time: 0.0052s\n",
            "Epoch: 0023 loss_train: 1.7488 acc_train: 0.3071 loss_val: 1.7413 acc_val: 0.3500 time: 0.0051s\n",
            "Epoch: 0024 loss_train: 1.7293 acc_train: 0.3000 loss_val: 1.7357 acc_val: 0.3500 time: 0.0053s\n",
            "Epoch: 0025 loss_train: 1.7287 acc_train: 0.3286 loss_val: 1.7303 acc_val: 0.3500 time: 0.0053s\n",
            "Epoch: 0026 loss_train: 1.7411 acc_train: 0.3214 loss_val: 1.7251 acc_val: 0.3500 time: 0.0053s\n",
            "Epoch: 0027 loss_train: 1.7307 acc_train: 0.3357 loss_val: 1.7200 acc_val: 0.3500 time: 0.0051s\n",
            "Epoch: 0028 loss_train: 1.7110 acc_train: 0.3214 loss_val: 1.7148 acc_val: 0.3500 time: 0.0054s\n",
            "Epoch: 0029 loss_train: 1.7099 acc_train: 0.3143 loss_val: 1.7096 acc_val: 0.3500 time: 0.0052s\n",
            "Epoch: 0030 loss_train: 1.6864 acc_train: 0.3714 loss_val: 1.7042 acc_val: 0.3500 time: 0.0052s\n",
            "Epoch: 0031 loss_train: 1.7112 acc_train: 0.3357 loss_val: 1.6988 acc_val: 0.3533 time: 0.0048s\n",
            "Epoch: 0032 loss_train: 1.6657 acc_train: 0.3214 loss_val: 1.6934 acc_val: 0.3533 time: 0.0053s\n",
            "Epoch: 0033 loss_train: 1.6649 acc_train: 0.3429 loss_val: 1.6880 acc_val: 0.3567 time: 0.0054s\n",
            "Epoch: 0034 loss_train: 1.6833 acc_train: 0.3286 loss_val: 1.6823 acc_val: 0.3633 time: 0.0050s\n",
            "Epoch: 0035 loss_train: 1.6489 acc_train: 0.3714 loss_val: 1.6765 acc_val: 0.3633 time: 0.0049s\n",
            "Epoch: 0036 loss_train: 1.6759 acc_train: 0.3643 loss_val: 1.6705 acc_val: 0.3633 time: 0.0051s\n",
            "Epoch: 0037 loss_train: 1.6577 acc_train: 0.3714 loss_val: 1.6643 acc_val: 0.3667 time: 0.0091s\n",
            "Epoch: 0038 loss_train: 1.6524 acc_train: 0.3929 loss_val: 1.6579 acc_val: 0.3667 time: 0.0076s\n",
            "Epoch: 0039 loss_train: 1.6286 acc_train: 0.3786 loss_val: 1.6511 acc_val: 0.3700 time: 0.0050s\n",
            "Epoch: 0040 loss_train: 1.5916 acc_train: 0.3929 loss_val: 1.6438 acc_val: 0.3800 time: 0.0052s\n",
            "Epoch: 0041 loss_train: 1.6074 acc_train: 0.4214 loss_val: 1.6362 acc_val: 0.3867 time: 0.0054s\n",
            "Epoch: 0042 loss_train: 1.6054 acc_train: 0.3929 loss_val: 1.6283 acc_val: 0.3867 time: 0.0052s\n",
            "Epoch: 0043 loss_train: 1.5846 acc_train: 0.4000 loss_val: 1.6203 acc_val: 0.3967 time: 0.0049s\n",
            "Epoch: 0044 loss_train: 1.5526 acc_train: 0.4071 loss_val: 1.6119 acc_val: 0.4033 time: 0.0049s\n",
            "Epoch: 0045 loss_train: 1.5413 acc_train: 0.4571 loss_val: 1.6033 acc_val: 0.4067 time: 0.0047s\n",
            "Epoch: 0046 loss_train: 1.5316 acc_train: 0.4071 loss_val: 1.5944 acc_val: 0.4100 time: 0.0057s\n",
            "Epoch: 0047 loss_train: 1.5011 acc_train: 0.4500 loss_val: 1.5852 acc_val: 0.4100 time: 0.0053s\n",
            "Epoch: 0048 loss_train: 1.5206 acc_train: 0.4214 loss_val: 1.5760 acc_val: 0.4100 time: 0.0050s\n",
            "Epoch: 0049 loss_train: 1.4846 acc_train: 0.4571 loss_val: 1.5667 acc_val: 0.4167 time: 0.0053s\n",
            "Epoch: 0050 loss_train: 1.4951 acc_train: 0.4571 loss_val: 1.5572 acc_val: 0.4333 time: 0.0053s\n",
            "Epoch: 0051 loss_train: 1.4674 acc_train: 0.4714 loss_val: 1.5474 acc_val: 0.4367 time: 0.0050s\n",
            "Epoch: 0052 loss_train: 1.4821 acc_train: 0.4571 loss_val: 1.5374 acc_val: 0.4467 time: 0.0055s\n",
            "Epoch: 0053 loss_train: 1.4796 acc_train: 0.4429 loss_val: 1.5273 acc_val: 0.4533 time: 0.0054s\n",
            "Epoch: 0054 loss_train: 1.4320 acc_train: 0.4786 loss_val: 1.5170 acc_val: 0.4633 time: 0.0053s\n",
            "Epoch: 0055 loss_train: 1.4122 acc_train: 0.4929 loss_val: 1.5063 acc_val: 0.4700 time: 0.0050s\n",
            "Epoch: 0056 loss_train: 1.4281 acc_train: 0.4786 loss_val: 1.4956 acc_val: 0.4700 time: 0.0052s\n",
            "Epoch: 0057 loss_train: 1.3996 acc_train: 0.4786 loss_val: 1.4848 acc_val: 0.4767 time: 0.0052s\n",
            "Epoch: 0058 loss_train: 1.3611 acc_train: 0.4857 loss_val: 1.4739 acc_val: 0.4900 time: 0.0056s\n",
            "Epoch: 0059 loss_train: 1.3707 acc_train: 0.5214 loss_val: 1.4627 acc_val: 0.5000 time: 0.0051s\n",
            "Epoch: 0060 loss_train: 1.3385 acc_train: 0.5429 loss_val: 1.4510 acc_val: 0.5167 time: 0.0053s\n",
            "Epoch: 0061 loss_train: 1.3532 acc_train: 0.5643 loss_val: 1.4389 acc_val: 0.5200 time: 0.0055s\n",
            "Epoch: 0062 loss_train: 1.3046 acc_train: 0.5500 loss_val: 1.4263 acc_val: 0.5267 time: 0.0070s\n",
            "Epoch: 0063 loss_train: 1.3282 acc_train: 0.5571 loss_val: 1.4136 acc_val: 0.5267 time: 0.0050s\n",
            "Epoch: 0064 loss_train: 1.3140 acc_train: 0.5214 loss_val: 1.4009 acc_val: 0.5367 time: 0.0089s\n",
            "Epoch: 0065 loss_train: 1.3145 acc_train: 0.5357 loss_val: 1.3883 acc_val: 0.5400 time: 0.0054s\n",
            "Epoch: 0066 loss_train: 1.2621 acc_train: 0.6000 loss_val: 1.3763 acc_val: 0.5467 time: 0.0061s\n",
            "Epoch: 0067 loss_train: 1.2356 acc_train: 0.5714 loss_val: 1.3646 acc_val: 0.5567 time: 0.0051s\n",
            "Epoch: 0068 loss_train: 1.2319 acc_train: 0.6071 loss_val: 1.3531 acc_val: 0.5700 time: 0.0050s\n",
            "Epoch: 0069 loss_train: 1.2307 acc_train: 0.6071 loss_val: 1.3418 acc_val: 0.5833 time: 0.0051s\n",
            "Epoch: 0070 loss_train: 1.2387 acc_train: 0.6214 loss_val: 1.3305 acc_val: 0.5933 time: 0.0087s\n",
            "Epoch: 0071 loss_train: 1.2121 acc_train: 0.6429 loss_val: 1.3196 acc_val: 0.6133 time: 0.0067s\n",
            "Epoch: 0072 loss_train: 1.1916 acc_train: 0.6214 loss_val: 1.3089 acc_val: 0.6367 time: 0.0052s\n",
            "Epoch: 0073 loss_train: 1.1901 acc_train: 0.6643 loss_val: 1.2986 acc_val: 0.6600 time: 0.0055s\n",
            "Epoch: 0074 loss_train: 1.1493 acc_train: 0.6500 loss_val: 1.2882 acc_val: 0.6667 time: 0.0052s\n",
            "Epoch: 0075 loss_train: 1.1563 acc_train: 0.6857 loss_val: 1.2777 acc_val: 0.6833 time: 0.0051s\n",
            "Epoch: 0076 loss_train: 1.1188 acc_train: 0.7143 loss_val: 1.2668 acc_val: 0.6933 time: 0.0053s\n",
            "Epoch: 0077 loss_train: 1.1222 acc_train: 0.6786 loss_val: 1.2558 acc_val: 0.7000 time: 0.0050s\n",
            "Epoch: 0078 loss_train: 1.1086 acc_train: 0.7214 loss_val: 1.2451 acc_val: 0.7000 time: 0.0052s\n",
            "Epoch: 0079 loss_train: 1.1062 acc_train: 0.7500 loss_val: 1.2340 acc_val: 0.7033 time: 0.0055s\n",
            "Epoch: 0080 loss_train: 1.0892 acc_train: 0.7214 loss_val: 1.2233 acc_val: 0.7167 time: 0.0055s\n",
            "Epoch: 0081 loss_train: 1.0634 acc_train: 0.7286 loss_val: 1.2126 acc_val: 0.7167 time: 0.0049s\n",
            "Epoch: 0082 loss_train: 1.0642 acc_train: 0.7214 loss_val: 1.2021 acc_val: 0.7200 time: 0.0050s\n",
            "Epoch: 0083 loss_train: 1.0202 acc_train: 0.7500 loss_val: 1.1916 acc_val: 0.7200 time: 0.0051s\n",
            "Epoch: 0084 loss_train: 1.0405 acc_train: 0.7643 loss_val: 1.1812 acc_val: 0.7200 time: 0.0048s\n",
            "Epoch: 0085 loss_train: 0.9897 acc_train: 0.7500 loss_val: 1.1712 acc_val: 0.7200 time: 0.0052s\n",
            "Epoch: 0086 loss_train: 0.9878 acc_train: 0.7214 loss_val: 1.1622 acc_val: 0.7200 time: 0.0053s\n",
            "Epoch: 0087 loss_train: 1.0421 acc_train: 0.7357 loss_val: 1.1540 acc_val: 0.7200 time: 0.0053s\n",
            "Epoch: 0088 loss_train: 1.0006 acc_train: 0.7571 loss_val: 1.1469 acc_val: 0.7267 time: 0.0054s\n",
            "Epoch: 0089 loss_train: 0.9638 acc_train: 0.7429 loss_val: 1.1400 acc_val: 0.7333 time: 0.0055s\n",
            "Epoch: 0090 loss_train: 0.9873 acc_train: 0.7571 loss_val: 1.1322 acc_val: 0.7333 time: 0.0051s\n",
            "Epoch: 0091 loss_train: 0.9373 acc_train: 0.8143 loss_val: 1.1235 acc_val: 0.7300 time: 0.0047s\n",
            "Epoch: 0092 loss_train: 0.9504 acc_train: 0.7929 loss_val: 1.1143 acc_val: 0.7367 time: 0.0055s\n",
            "Epoch: 0093 loss_train: 0.9099 acc_train: 0.7286 loss_val: 1.1057 acc_val: 0.7433 time: 0.0056s\n",
            "Epoch: 0094 loss_train: 0.9249 acc_train: 0.7714 loss_val: 1.0972 acc_val: 0.7433 time: 0.0052s\n",
            "Epoch: 0095 loss_train: 0.9044 acc_train: 0.7714 loss_val: 1.0888 acc_val: 0.7400 time: 0.0048s\n",
            "Epoch: 0096 loss_train: 0.8860 acc_train: 0.7571 loss_val: 1.0809 acc_val: 0.7433 time: 0.0047s\n",
            "Epoch: 0097 loss_train: 0.9192 acc_train: 0.7786 loss_val: 1.0736 acc_val: 0.7433 time: 0.0050s\n",
            "Epoch: 0098 loss_train: 0.8509 acc_train: 0.8071 loss_val: 1.0662 acc_val: 0.7433 time: 0.0048s\n",
            "Epoch: 0099 loss_train: 0.8375 acc_train: 0.8357 loss_val: 1.0584 acc_val: 0.7500 time: 0.0048s\n",
            "Epoch: 0100 loss_train: 0.8740 acc_train: 0.7571 loss_val: 1.0507 acc_val: 0.7533 time: 0.0052s\n",
            "Epoch: 0101 loss_train: 0.8779 acc_train: 0.7571 loss_val: 1.0435 acc_val: 0.7567 time: 0.0055s\n",
            "Epoch: 0102 loss_train: 0.8655 acc_train: 0.8071 loss_val: 1.0376 acc_val: 0.7600 time: 0.0048s\n",
            "Epoch: 0103 loss_train: 0.8181 acc_train: 0.8214 loss_val: 1.0321 acc_val: 0.7700 time: 0.0052s\n",
            "Epoch: 0104 loss_train: 0.8401 acc_train: 0.8000 loss_val: 1.0266 acc_val: 0.7700 time: 0.0052s\n",
            "Epoch: 0105 loss_train: 0.8333 acc_train: 0.8429 loss_val: 1.0203 acc_val: 0.7733 time: 0.0051s\n",
            "Epoch: 0106 loss_train: 0.8403 acc_train: 0.8000 loss_val: 1.0137 acc_val: 0.7733 time: 0.0069s\n",
            "Epoch: 0107 loss_train: 0.8138 acc_train: 0.8214 loss_val: 1.0069 acc_val: 0.7833 time: 0.0074s\n",
            "Epoch: 0108 loss_train: 0.8248 acc_train: 0.8071 loss_val: 1.0001 acc_val: 0.7867 time: 0.0050s\n",
            "Epoch: 0109 loss_train: 0.8228 acc_train: 0.8214 loss_val: 0.9945 acc_val: 0.7800 time: 0.0051s\n",
            "Epoch: 0110 loss_train: 0.7814 acc_train: 0.8357 loss_val: 0.9896 acc_val: 0.7800 time: 0.0052s\n",
            "Epoch: 0111 loss_train: 0.7797 acc_train: 0.8500 loss_val: 0.9848 acc_val: 0.7800 time: 0.0053s\n",
            "Epoch: 0112 loss_train: 0.7530 acc_train: 0.8500 loss_val: 0.9796 acc_val: 0.7800 time: 0.0050s\n",
            "Epoch: 0113 loss_train: 0.8091 acc_train: 0.8000 loss_val: 0.9739 acc_val: 0.7800 time: 0.0049s\n",
            "Epoch: 0114 loss_train: 0.7898 acc_train: 0.8500 loss_val: 0.9681 acc_val: 0.7800 time: 0.0050s\n",
            "Epoch: 0115 loss_train: 0.7515 acc_train: 0.8429 loss_val: 0.9623 acc_val: 0.7867 time: 0.0052s\n",
            "Epoch: 0116 loss_train: 0.7839 acc_train: 0.8429 loss_val: 0.9569 acc_val: 0.7833 time: 0.0054s\n",
            "Epoch: 0117 loss_train: 0.7987 acc_train: 0.8286 loss_val: 0.9521 acc_val: 0.7767 time: 0.0051s\n",
            "Epoch: 0118 loss_train: 0.7292 acc_train: 0.8286 loss_val: 0.9477 acc_val: 0.7733 time: 0.0051s\n",
            "Epoch: 0119 loss_train: 0.7649 acc_train: 0.8286 loss_val: 0.9427 acc_val: 0.7800 time: 0.0049s\n",
            "Epoch: 0120 loss_train: 0.7509 acc_train: 0.8143 loss_val: 0.9383 acc_val: 0.7800 time: 0.0049s\n",
            "Epoch: 0121 loss_train: 0.7670 acc_train: 0.8571 loss_val: 0.9343 acc_val: 0.7833 time: 0.0048s\n",
            "Epoch: 0122 loss_train: 0.6947 acc_train: 0.8714 loss_val: 0.9302 acc_val: 0.7833 time: 0.0051s\n",
            "Epoch: 0123 loss_train: 0.7045 acc_train: 0.8500 loss_val: 0.9253 acc_val: 0.7833 time: 0.0052s\n",
            "Epoch: 0124 loss_train: 0.7398 acc_train: 0.8429 loss_val: 0.9205 acc_val: 0.7967 time: 0.0052s\n",
            "Epoch: 0125 loss_train: 0.6658 acc_train: 0.8500 loss_val: 0.9166 acc_val: 0.7967 time: 0.0054s\n",
            "Epoch: 0126 loss_train: 0.7140 acc_train: 0.8643 loss_val: 0.9125 acc_val: 0.7967 time: 0.0055s\n",
            "Epoch: 0127 loss_train: 0.7135 acc_train: 0.8357 loss_val: 0.9082 acc_val: 0.7967 time: 0.0049s\n",
            "Epoch: 0128 loss_train: 0.6932 acc_train: 0.8214 loss_val: 0.9044 acc_val: 0.7967 time: 0.0052s\n",
            "Epoch: 0129 loss_train: 0.6529 acc_train: 0.8929 loss_val: 0.9003 acc_val: 0.7967 time: 0.0049s\n",
            "Epoch: 0130 loss_train: 0.7148 acc_train: 0.8500 loss_val: 0.8960 acc_val: 0.7967 time: 0.0051s\n",
            "Epoch: 0131 loss_train: 0.6776 acc_train: 0.8571 loss_val: 0.8911 acc_val: 0.7967 time: 0.0048s\n",
            "Epoch: 0132 loss_train: 0.6939 acc_train: 0.8786 loss_val: 0.8864 acc_val: 0.7967 time: 0.0051s\n",
            "Epoch: 0133 loss_train: 0.6990 acc_train: 0.8429 loss_val: 0.8822 acc_val: 0.7967 time: 0.0047s\n",
            "Epoch: 0134 loss_train: 0.6928 acc_train: 0.8429 loss_val: 0.8785 acc_val: 0.7967 time: 0.0048s\n",
            "Epoch: 0135 loss_train: 0.6438 acc_train: 0.8429 loss_val: 0.8757 acc_val: 0.7967 time: 0.0049s\n",
            "Epoch: 0136 loss_train: 0.6535 acc_train: 0.8786 loss_val: 0.8726 acc_val: 0.8000 time: 0.0055s\n",
            "Epoch: 0137 loss_train: 0.6801 acc_train: 0.8500 loss_val: 0.8700 acc_val: 0.8000 time: 0.0049s\n",
            "Epoch: 0138 loss_train: 0.6432 acc_train: 0.8929 loss_val: 0.8669 acc_val: 0.8000 time: 0.0051s\n",
            "Epoch: 0139 loss_train: 0.6730 acc_train: 0.8214 loss_val: 0.8633 acc_val: 0.8033 time: 0.0050s\n",
            "Epoch: 0140 loss_train: 0.6757 acc_train: 0.8357 loss_val: 0.8598 acc_val: 0.7967 time: 0.0048s\n",
            "Epoch: 0141 loss_train: 0.6964 acc_train: 0.8286 loss_val: 0.8569 acc_val: 0.8100 time: 0.0051s\n",
            "Epoch: 0142 loss_train: 0.6327 acc_train: 0.8500 loss_val: 0.8542 acc_val: 0.8000 time: 0.0069s\n",
            "Epoch: 0143 loss_train: 0.6483 acc_train: 0.8929 loss_val: 0.8529 acc_val: 0.8033 time: 0.0073s\n",
            "Epoch: 0144 loss_train: 0.6343 acc_train: 0.8857 loss_val: 0.8511 acc_val: 0.8000 time: 0.0058s\n",
            "Epoch: 0145 loss_train: 0.6976 acc_train: 0.8357 loss_val: 0.8477 acc_val: 0.8000 time: 0.0058s\n",
            "Epoch: 0146 loss_train: 0.6182 acc_train: 0.8714 loss_val: 0.8443 acc_val: 0.8033 time: 0.0056s\n",
            "Epoch: 0147 loss_train: 0.6160 acc_train: 0.9071 loss_val: 0.8409 acc_val: 0.8067 time: 0.0055s\n",
            "Epoch: 0148 loss_train: 0.6641 acc_train: 0.8643 loss_val: 0.8370 acc_val: 0.8033 time: 0.0059s\n",
            "Epoch: 0149 loss_train: 0.5856 acc_train: 0.9000 loss_val: 0.8331 acc_val: 0.8033 time: 0.0063s\n",
            "Epoch: 0150 loss_train: 0.6351 acc_train: 0.8429 loss_val: 0.8297 acc_val: 0.8033 time: 0.0058s\n",
            "Epoch: 0151 loss_train: 0.6024 acc_train: 0.8786 loss_val: 0.8267 acc_val: 0.8033 time: 0.0059s\n",
            "Epoch: 0152 loss_train: 0.5831 acc_train: 0.8714 loss_val: 0.8245 acc_val: 0.8033 time: 0.0056s\n",
            "Epoch: 0153 loss_train: 0.5692 acc_train: 0.8714 loss_val: 0.8229 acc_val: 0.8033 time: 0.0057s\n",
            "Epoch: 0154 loss_train: 0.5938 acc_train: 0.8786 loss_val: 0.8203 acc_val: 0.8033 time: 0.0059s\n",
            "Epoch: 0155 loss_train: 0.5684 acc_train: 0.9214 loss_val: 0.8174 acc_val: 0.8033 time: 0.0057s\n",
            "Epoch: 0156 loss_train: 0.5828 acc_train: 0.8857 loss_val: 0.8135 acc_val: 0.8033 time: 0.0055s\n",
            "Epoch: 0157 loss_train: 0.5579 acc_train: 0.8429 loss_val: 0.8102 acc_val: 0.8100 time: 0.0053s\n",
            "Epoch: 0158 loss_train: 0.5653 acc_train: 0.8857 loss_val: 0.8076 acc_val: 0.8067 time: 0.0082s\n",
            "Epoch: 0159 loss_train: 0.6043 acc_train: 0.8786 loss_val: 0.8055 acc_val: 0.8067 time: 0.0061s\n",
            "Epoch: 0160 loss_train: 0.5852 acc_train: 0.8500 loss_val: 0.8041 acc_val: 0.8033 time: 0.0057s\n",
            "Epoch: 0161 loss_train: 0.5940 acc_train: 0.8786 loss_val: 0.8026 acc_val: 0.8067 time: 0.0050s\n",
            "Epoch: 0162 loss_train: 0.5604 acc_train: 0.8929 loss_val: 0.8013 acc_val: 0.8033 time: 0.0055s\n",
            "Epoch: 0163 loss_train: 0.6382 acc_train: 0.8500 loss_val: 0.7997 acc_val: 0.8000 time: 0.0048s\n",
            "Epoch: 0164 loss_train: 0.5488 acc_train: 0.8857 loss_val: 0.7975 acc_val: 0.8067 time: 0.0050s\n",
            "Epoch: 0165 loss_train: 0.5204 acc_train: 0.9000 loss_val: 0.7952 acc_val: 0.8067 time: 0.0054s\n",
            "Epoch: 0166 loss_train: 0.5564 acc_train: 0.8929 loss_val: 0.7935 acc_val: 0.8067 time: 0.0049s\n",
            "Epoch: 0167 loss_train: 0.5941 acc_train: 0.8643 loss_val: 0.7898 acc_val: 0.8100 time: 0.0050s\n",
            "Epoch: 0168 loss_train: 0.5415 acc_train: 0.9000 loss_val: 0.7860 acc_val: 0.8133 time: 0.0049s\n",
            "Epoch: 0169 loss_train: 0.5627 acc_train: 0.8929 loss_val: 0.7834 acc_val: 0.8133 time: 0.0055s\n",
            "Epoch: 0170 loss_train: 0.5476 acc_train: 0.9143 loss_val: 0.7804 acc_val: 0.8167 time: 0.0050s\n",
            "Epoch: 0171 loss_train: 0.5224 acc_train: 0.9214 loss_val: 0.7786 acc_val: 0.8133 time: 0.0049s\n",
            "Epoch: 0172 loss_train: 0.5705 acc_train: 0.8714 loss_val: 0.7776 acc_val: 0.8067 time: 0.0053s\n",
            "Epoch: 0173 loss_train: 0.5826 acc_train: 0.8857 loss_val: 0.7752 acc_val: 0.8067 time: 0.0050s\n",
            "Epoch: 0174 loss_train: 0.5441 acc_train: 0.9000 loss_val: 0.7734 acc_val: 0.8067 time: 0.0050s\n",
            "Epoch: 0175 loss_train: 0.4942 acc_train: 0.9357 loss_val: 0.7713 acc_val: 0.8067 time: 0.0051s\n",
            "Epoch: 0176 loss_train: 0.5009 acc_train: 0.9214 loss_val: 0.7691 acc_val: 0.8100 time: 0.0080s\n",
            "Epoch: 0177 loss_train: 0.5568 acc_train: 0.8571 loss_val: 0.7666 acc_val: 0.8100 time: 0.0066s\n",
            "Epoch: 0178 loss_train: 0.5644 acc_train: 0.8929 loss_val: 0.7650 acc_val: 0.8167 time: 0.0056s\n",
            "Epoch: 0179 loss_train: 0.5454 acc_train: 0.8786 loss_val: 0.7638 acc_val: 0.8133 time: 0.0052s\n",
            "Epoch: 0180 loss_train: 0.4865 acc_train: 0.9143 loss_val: 0.7624 acc_val: 0.8167 time: 0.0047s\n",
            "Epoch: 0181 loss_train: 0.5597 acc_train: 0.8786 loss_val: 0.7613 acc_val: 0.8133 time: 0.0052s\n",
            "Epoch: 0182 loss_train: 0.4913 acc_train: 0.9000 loss_val: 0.7606 acc_val: 0.8133 time: 0.0052s\n",
            "Epoch: 0183 loss_train: 0.5081 acc_train: 0.8929 loss_val: 0.7606 acc_val: 0.8100 time: 0.0049s\n",
            "Epoch: 0184 loss_train: 0.5076 acc_train: 0.8929 loss_val: 0.7598 acc_val: 0.8100 time: 0.0050s\n",
            "Epoch: 0185 loss_train: 0.5798 acc_train: 0.8714 loss_val: 0.7582 acc_val: 0.8100 time: 0.0053s\n",
            "Epoch: 0186 loss_train: 0.4722 acc_train: 0.9143 loss_val: 0.7567 acc_val: 0.8067 time: 0.0056s\n",
            "Epoch: 0187 loss_train: 0.5519 acc_train: 0.8929 loss_val: 0.7532 acc_val: 0.8100 time: 0.0054s\n",
            "Epoch: 0188 loss_train: 0.5487 acc_train: 0.9071 loss_val: 0.7516 acc_val: 0.8100 time: 0.0052s\n",
            "Epoch: 0189 loss_train: 0.5053 acc_train: 0.9143 loss_val: 0.7502 acc_val: 0.8167 time: 0.0053s\n",
            "Epoch: 0190 loss_train: 0.4816 acc_train: 0.9000 loss_val: 0.7491 acc_val: 0.8133 time: 0.0052s\n",
            "Epoch: 0191 loss_train: 0.4872 acc_train: 0.8929 loss_val: 0.7478 acc_val: 0.8100 time: 0.0051s\n",
            "Epoch: 0192 loss_train: 0.4605 acc_train: 0.8857 loss_val: 0.7476 acc_val: 0.8100 time: 0.0048s\n",
            "Epoch: 0193 loss_train: 0.5285 acc_train: 0.9143 loss_val: 0.7474 acc_val: 0.8133 time: 0.0055s\n",
            "Epoch: 0194 loss_train: 0.4644 acc_train: 0.9000 loss_val: 0.7480 acc_val: 0.8100 time: 0.0051s\n",
            "Epoch: 0195 loss_train: 0.5040 acc_train: 0.8929 loss_val: 0.7479 acc_val: 0.8133 time: 0.0052s\n",
            "Epoch: 0196 loss_train: 0.4611 acc_train: 0.9357 loss_val: 0.7478 acc_val: 0.8133 time: 0.0053s\n",
            "Epoch: 0197 loss_train: 0.4940 acc_train: 0.9143 loss_val: 0.7458 acc_val: 0.8167 time: 0.0056s\n",
            "Epoch: 0198 loss_train: 0.3966 acc_train: 0.9500 loss_val: 0.7422 acc_val: 0.8200 time: 0.0054s\n",
            "Epoch: 0199 loss_train: 0.4701 acc_train: 0.9071 loss_val: 0.7396 acc_val: 0.8133 time: 0.0050s\n",
            "Epoch: 0200 loss_train: 0.4838 acc_train: 0.9143 loss_val: 0.7363 acc_val: 0.8200 time: 0.0055s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 1.3386s\n",
            "Test set results: loss= 0.7836 accuracy= 0.8130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tU8y4DYqohN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}