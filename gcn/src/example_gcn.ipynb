{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN example code for Animesh\n",
    "### PyTorch backend explicit set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN Class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "\"\"\"GCN using DGL nn package\n",
    "References:\n",
    "- Semi-Supervised Classification with Graph Convolutional Networks\n",
    "- Paper: https://arxiv.org/abs/1609.02907\n",
    "- Code: https://github.com/tkipf/gcn\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.g = g\n",
    "        self.layers = nn.ModuleList()\n",
    "        # input layer\n",
    "        self.layers.append(GraphConv(in_feats, n_hidden, activation=activation))\n",
    "        # hidden layers\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.append(GraphConv(n_hidden, n_hidden, activation=activation))\n",
    "        # output layer\n",
    "        self.layers.append(GraphConv(n_hidden, n_classes))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, features):\n",
    "        h = features\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i != 0:\n",
    "                h = self.dropout(h)\n",
    "            h = layer(self.g, h)\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argument setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(dropout=0.5, fold=10, gpu=-1, lr=0.01, metric='euclidean', n_epochs=100, n_hidden=16, n_layers=1, number_edges=10, self_loop=False, weight_decay=0.0005)\n"
     ]
    }
   ],
   "source": [
    "import argparse, time\n",
    "import numpy as np\n",
    "from dgl.data import register_data_args, load_data\n",
    "\n",
    "parser = argparse.ArgumentParser(description='GCN')\n",
    "#register_data_args(parser)\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.5,\n",
    "        help=\"dropout probability\")\n",
    "parser.add_argument(\"--gpu\", type=int, default=-1,\n",
    "        help=\"gpu\")\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-2,\n",
    "        help=\"learning rate\")\n",
    "parser.add_argument(\"--n-epochs\", type=int, default=100,\n",
    "        help=\"number of training epochs\")\n",
    "parser.add_argument(\"--n-hidden\", type=int, default=16,\n",
    "        help=\"number of hidden gcn units\")\n",
    "parser.add_argument(\"--n-layers\", type=int, default=1,\n",
    "        help=\"number of hidden gcn layers\")\n",
    "parser.add_argument(\"--weight-decay\", type=float, default=5e-4,\n",
    "        help=\"Weight for L2 loss\")\n",
    "parser.add_argument(\"--fold\", type=float, default=10,\n",
    "        help=\"Weight for L2 loss\")\n",
    "parser.add_argument(\"--number_edges\", type=int, default=10,\n",
    "                    help=\"Graph: minimum number of edges per vertex.\")\n",
    "parser.add_argument(\"--metric\", type=str, default='euclidean',\n",
    "                    help=\"Graph: similarity measure (between features).\")\n",
    "parser.add_argument(\"--self-loop\", action='store_true',\n",
    "        help=\"graph self-loop (default=False)\")\n",
    "parser.set_defaults(self_loop=False)\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supporting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, features, labels, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)\n",
    "    \n",
    "def grid_graph(m, corners=False):\n",
    "    z = graph.grid(m)\n",
    "    dist, idx = graph.distance_sklearn_metrics(z, k=args.number_edges, metric=args.metric)\n",
    "    A = graph.adjacency(dist, idx)\n",
    "\n",
    "    # Connections are only vertical or horizontal on the grid.\n",
    "    # Corner vertices are connected to 2 neightbors only.\n",
    "    if corners:\n",
    "        import scipy.sparse\n",
    "        A = A.toarray()\n",
    "        A[A < A.max()/1.5] = 0\n",
    "        A = scipy.sparse.csr_matrix(A)\n",
    "        print('{} edges'.format(A.nnz))\n",
    "    print(\"{} > {} edges\".format(A.nnz//2, args.number_edges*m**2//2))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy import stats\n",
    "from dgl import DGLGraph\n",
    "\n",
    "features = np.loadtxt(open(\"../100_final_data.csv\", \"rb\"), delimiter=\",\", skiprows=1, usecols=range(1,82) )\n",
    "features = features.transpose()\n",
    "features = stats.zscore(features)\n",
    "y = np.loadtxt(open(\"../all_data.csv\", \"rb\"), delimiter=\",\", skiprows=1, usecols=range(1,82))\n",
    "y = y[0,:]\n",
    "np_features = features.copy()\n",
    "bin_ixs = []\n",
    "train_ixs = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=args.fold)\n",
    "for train_index, test_index in skf.split(features, y):\n",
    "    bin_ixs.append(test_index)\n",
    "    train_ixs.append(train_index)\n",
    "\n",
    "features = torch.FloatTensor(features)\n",
    "y = torch.LongTensor(y)    \n",
    "train_index = torch.from_numpy(train_index)\n",
    "test_index = torch.from_numpy(test_index)\n",
    "\n",
    "# set train and test mask\n",
    "train_mask = np.zeros(y.shape, dtype=bool)\n",
    "test_mask =  np.zeros(y.shape, dtype=bool)\n",
    "train_mask[train_index] = True\n",
    "test_mask[test_index] = True\n",
    "val_mask = test_mask # this is not correct but just make sure we can pass through\n",
    "train_mask = torch.BoolTensor(train_mask)\n",
    "test_mask = torch.BoolTensor(test_mask)\n",
    "val_mask =  torch.BoolTensor(val_mask)\n",
    "in_feats = features.shape[1]\n",
    "n_classes = 2\n",
    "labels = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct graph <--- this is not a good graph as it's only for image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470 > 405 edges\n",
      "----Data statistics------'\n",
      "  #Edges 940\n",
      "  #Classes 2\n",
      "  #Train samples 73\n",
      "  #Val samples 8\n",
      "  #Test samples 8\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, '..')\n",
    "from lib2 import graph\n",
    "n_edges = 10\n",
    "t_start = time.process_time()\n",
    "A = grid_graph(9, corners=False)\n",
    "A = graph.replace_random_edges(A, 0)\n",
    "#graphs, perm = coarsening.coarsen(A, levels=args.coarsening_levels, self_connections=False)\n",
    "#L = [graph.laplacian(A, normalized=True) for A in graphs]\n",
    "g = DGLGraph()\n",
    "g.from_scipy_sparse_matrix(A)\n",
    "n_edges = A.nnz\n",
    "\n",
    "print(\"\"\"----Data statistics------'\n",
    "  #Edges %d\n",
    "  #Classes %d\n",
    "  #Train samples %d\n",
    "  #Val samples %d\n",
    "  #Test samples %d\"\"\" %\n",
    "      (n_edges, n_classes,\n",
    "          train_mask.int().sum().item(),\n",
    "          val_mask.int().sum().item(),\n",
    "          test_mask.int().sum().item()))\n",
    "\n",
    "\n",
    "if args.gpu < 0:\n",
    "    cuda = False\n",
    "else:\n",
    "    cuda = True\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    features = features.cuda()\n",
    "    labels = labels.cuda()\n",
    "    train_mask = train_mask.cuda()\n",
    "    val_mask = val_mask.cuda()\n",
    "    test_mask = test_mask.cuda()\n",
    "\n",
    "# add self loop\n",
    "if args.self_loop:\n",
    "    g.remove_edges_from(nx.selfloop_edges(g))\n",
    "    g.add_edges_from(zip(g.nodes(), g.nodes()))\n",
    "    \n",
    "if cuda:\n",
    "    g = g.to(args.gpu)\n",
    "\n",
    "# Graph normalization\n",
    "degs = g.in_degrees().float()\n",
    "norm = torch.pow(degs, -0.5)\n",
    "norm[torch.isinf(norm)] = 0\n",
    "if cuda:\n",
    "    norm = norm.cuda()\n",
    "g.ndata['norm'] = norm.unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's build GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjyoo/anaconda3/envs/dgl/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/sjyoo/anaconda3/envs/dgl/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Time(s) nan | Loss 0.7245 | Accuracy 0.3750 | ETputs(KTEPS) nan\n",
      "Epoch 00001 | Time(s) nan | Loss 0.6749 | Accuracy 0.3750 | ETputs(KTEPS) nan\n",
      "Epoch 00002 | Time(s) nan | Loss 0.6494 | Accuracy 0.3750 | ETputs(KTEPS) nan\n",
      "Epoch 00003 | Time(s) 0.0046 | Loss 0.6268 | Accuracy 0.3750 | ETputs(KTEPS) 202.73\n",
      "Epoch 00004 | Time(s) 0.0051 | Loss 0.6103 | Accuracy 0.3750 | ETputs(KTEPS) 183.46\n",
      "Epoch 00005 | Time(s) 0.0053 | Loss 0.6030 | Accuracy 0.3750 | ETputs(KTEPS) 177.32\n",
      "Epoch 00006 | Time(s) 0.0050 | Loss 0.5845 | Accuracy 0.3750 | ETputs(KTEPS) 186.45\n",
      "Epoch 00007 | Time(s) 0.0049 | Loss 0.5681 | Accuracy 0.3750 | ETputs(KTEPS) 192.91\n",
      "Epoch 00008 | Time(s) 0.0047 | Loss 0.5751 | Accuracy 0.3750 | ETputs(KTEPS) 199.90\n",
      "Epoch 00009 | Time(s) 0.0046 | Loss 0.5585 | Accuracy 0.3750 | ETputs(KTEPS) 204.34\n",
      "Epoch 00010 | Time(s) 0.0047 | Loss 0.5662 | Accuracy 0.3750 | ETputs(KTEPS) 201.45\n",
      "Epoch 00011 | Time(s) 0.0046 | Loss 0.5587 | Accuracy 0.3750 | ETputs(KTEPS) 202.62\n",
      "Epoch 00012 | Time(s) 0.0046 | Loss 0.5503 | Accuracy 0.3750 | ETputs(KTEPS) 205.87\n",
      "Epoch 00013 | Time(s) 0.0045 | Loss 0.5426 | Accuracy 0.3750 | ETputs(KTEPS) 207.06\n",
      "Epoch 00014 | Time(s) 0.0046 | Loss 0.5638 | Accuracy 0.3750 | ETputs(KTEPS) 206.19\n",
      "Epoch 00015 | Time(s) 0.0046 | Loss 0.5607 | Accuracy 0.3750 | ETputs(KTEPS) 206.59\n",
      "Epoch 00016 | Time(s) 0.0046 | Loss 0.5707 | Accuracy 0.3750 | ETputs(KTEPS) 205.48\n",
      "Epoch 00017 | Time(s) 0.0045 | Loss 0.5493 | Accuracy 0.3750 | ETputs(KTEPS) 207.33\n",
      "Epoch 00018 | Time(s) 0.0046 | Loss 0.5567 | Accuracy 0.3750 | ETputs(KTEPS) 206.03\n",
      "Epoch 00019 | Time(s) 0.0046 | Loss 0.5542 | Accuracy 0.3750 | ETputs(KTEPS) 206.40\n",
      "Epoch 00020 | Time(s) 0.0045 | Loss 0.5386 | Accuracy 0.3750 | ETputs(KTEPS) 207.42\n",
      "Epoch 00021 | Time(s) 0.0046 | Loss 0.5516 | Accuracy 0.3750 | ETputs(KTEPS) 206.35\n",
      "Epoch 00022 | Time(s) 0.0045 | Loss 0.5550 | Accuracy 0.3750 | ETputs(KTEPS) 207.70\n",
      "Epoch 00023 | Time(s) 0.0045 | Loss 0.5410 | Accuracy 0.3750 | ETputs(KTEPS) 208.65\n",
      "Epoch 00024 | Time(s) 0.0045 | Loss 0.5565 | Accuracy 0.3750 | ETputs(KTEPS) 209.82\n",
      "Epoch 00025 | Time(s) 0.0045 | Loss 0.5313 | Accuracy 0.3750 | ETputs(KTEPS) 209.63\n",
      "Epoch 00026 | Time(s) 0.0045 | Loss 0.5170 | Accuracy 0.3750 | ETputs(KTEPS) 210.92\n",
      "Epoch 00027 | Time(s) 0.0044 | Loss 0.5289 | Accuracy 0.3750 | ETputs(KTEPS) 211.83\n",
      "Epoch 00028 | Time(s) 0.0045 | Loss 0.5351 | Accuracy 0.3750 | ETputs(KTEPS) 210.00\n",
      "Epoch 00029 | Time(s) 0.0045 | Loss 0.5296 | Accuracy 0.3750 | ETputs(KTEPS) 209.89\n",
      "Epoch 00030 | Time(s) 0.0045 | Loss 0.5306 | Accuracy 0.3750 | ETputs(KTEPS) 209.41\n",
      "Epoch 00031 | Time(s) 0.0045 | Loss 0.5320 | Accuracy 0.3750 | ETputs(KTEPS) 210.04\n",
      "Epoch 00032 | Time(s) 0.0045 | Loss 0.5319 | Accuracy 0.3750 | ETputs(KTEPS) 209.72\n",
      "Epoch 00033 | Time(s) 0.0045 | Loss 0.5140 | Accuracy 0.3750 | ETputs(KTEPS) 206.84\n",
      "Epoch 00034 | Time(s) 0.0045 | Loss 0.5162 | Accuracy 0.3750 | ETputs(KTEPS) 207.36\n",
      "Epoch 00035 | Time(s) 0.0045 | Loss 0.5316 | Accuracy 0.3750 | ETputs(KTEPS) 208.23\n",
      "Epoch 00036 | Time(s) 0.0045 | Loss 0.5305 | Accuracy 0.3750 | ETputs(KTEPS) 206.97\n",
      "Epoch 00037 | Time(s) 0.0045 | Loss 0.5247 | Accuracy 0.3750 | ETputs(KTEPS) 207.78\n",
      "Epoch 00038 | Time(s) 0.0045 | Loss 0.5155 | Accuracy 0.3750 | ETputs(KTEPS) 206.83\n",
      "Epoch 00039 | Time(s) 0.0045 | Loss 0.5230 | Accuracy 0.3750 | ETputs(KTEPS) 207.26\n",
      "Epoch 00040 | Time(s) 0.0045 | Loss 0.5384 | Accuracy 0.3750 | ETputs(KTEPS) 207.94\n",
      "Epoch 00041 | Time(s) 0.0045 | Loss 0.5134 | Accuracy 0.3750 | ETputs(KTEPS) 208.18\n",
      "Epoch 00042 | Time(s) 0.0045 | Loss 0.5179 | Accuracy 0.3750 | ETputs(KTEPS) 208.87\n",
      "Epoch 00043 | Time(s) 0.0045 | Loss 0.5147 | Accuracy 0.3750 | ETputs(KTEPS) 209.16\n",
      "Epoch 00044 | Time(s) 0.0045 | Loss 0.5268 | Accuracy 0.3750 | ETputs(KTEPS) 209.78\n",
      "Epoch 00045 | Time(s) 0.0045 | Loss 0.5203 | Accuracy 0.3750 | ETputs(KTEPS) 210.43\n",
      "Epoch 00046 | Time(s) 0.0045 | Loss 0.5130 | Accuracy 0.3750 | ETputs(KTEPS) 210.21\n",
      "Epoch 00047 | Time(s) 0.0045 | Loss 0.5334 | Accuracy 0.3750 | ETputs(KTEPS) 210.73\n",
      "Epoch 00048 | Time(s) 0.0045 | Loss 0.5202 | Accuracy 0.3750 | ETputs(KTEPS) 211.03\n",
      "Epoch 00049 | Time(s) 0.0044 | Loss 0.5270 | Accuracy 0.3750 | ETputs(KTEPS) 211.44\n",
      "Epoch 00050 | Time(s) 0.0044 | Loss 0.5237 | Accuracy 0.3750 | ETputs(KTEPS) 211.94\n",
      "Epoch 00051 | Time(s) 0.0044 | Loss 0.5060 | Accuracy 0.3750 | ETputs(KTEPS) 211.68\n",
      "Epoch 00052 | Time(s) 0.0044 | Loss 0.5012 | Accuracy 0.3750 | ETputs(KTEPS) 212.25\n",
      "Epoch 00053 | Time(s) 0.0044 | Loss 0.5192 | Accuracy 0.3750 | ETputs(KTEPS) 212.28\n",
      "Epoch 00054 | Time(s) 0.0044 | Loss 0.5132 | Accuracy 0.3750 | ETputs(KTEPS) 212.69\n",
      "Epoch 00055 | Time(s) 0.0044 | Loss 0.4991 | Accuracy 0.3750 | ETputs(KTEPS) 213.07\n",
      "Epoch 00056 | Time(s) 0.0044 | Loss 0.4966 | Accuracy 0.3750 | ETputs(KTEPS) 212.34\n",
      "Epoch 00057 | Time(s) 0.0044 | Loss 0.5029 | Accuracy 0.3750 | ETputs(KTEPS) 211.58\n",
      "Epoch 00058 | Time(s) 0.0045 | Loss 0.5114 | Accuracy 0.3750 | ETputs(KTEPS) 210.15\n",
      "Epoch 00059 | Time(s) 0.0045 | Loss 0.5040 | Accuracy 0.3750 | ETputs(KTEPS) 207.94\n",
      "Epoch 00060 | Time(s) 0.0045 | Loss 0.4905 | Accuracy 0.3750 | ETputs(KTEPS) 207.95\n",
      "Epoch 00061 | Time(s) 0.0045 | Loss 0.5026 | Accuracy 0.3750 | ETputs(KTEPS) 207.45\n",
      "Epoch 00062 | Time(s) 0.0045 | Loss 0.5019 | Accuracy 0.3750 | ETputs(KTEPS) 206.78\n",
      "Epoch 00063 | Time(s) 0.0046 | Loss 0.5112 | Accuracy 0.3750 | ETputs(KTEPS) 205.90\n",
      "Epoch 00064 | Time(s) 0.0046 | Loss 0.5025 | Accuracy 0.3750 | ETputs(KTEPS) 205.72\n",
      "Epoch 00065 | Time(s) 0.0046 | Loss 0.4899 | Accuracy 0.3750 | ETputs(KTEPS) 205.58\n",
      "Epoch 00066 | Time(s) 0.0046 | Loss 0.4909 | Accuracy 0.3750 | ETputs(KTEPS) 205.19\n",
      "Epoch 00067 | Time(s) 0.0046 | Loss 0.5166 | Accuracy 0.3750 | ETputs(KTEPS) 205.48\n",
      "Epoch 00068 | Time(s) 0.0046 | Loss 0.5041 | Accuracy 0.3750 | ETputs(KTEPS) 205.55\n",
      "Epoch 00069 | Time(s) 0.0046 | Loss 0.5067 | Accuracy 0.3750 | ETputs(KTEPS) 205.88\n",
      "Epoch 00070 | Time(s) 0.0046 | Loss 0.4952 | Accuracy 0.3750 | ETputs(KTEPS) 205.73\n",
      "Epoch 00071 | Time(s) 0.0046 | Loss 0.4971 | Accuracy 0.3750 | ETputs(KTEPS) 205.98\n",
      "Epoch 00072 | Time(s) 0.0046 | Loss 0.4929 | Accuracy 0.3750 | ETputs(KTEPS) 205.83\n",
      "Epoch 00073 | Time(s) 0.0046 | Loss 0.5204 | Accuracy 0.3750 | ETputs(KTEPS) 205.74\n",
      "Epoch 00074 | Time(s) 0.0046 | Loss 0.4960 | Accuracy 0.3750 | ETputs(KTEPS) 205.91\n",
      "Epoch 00075 | Time(s) 0.0046 | Loss 0.4870 | Accuracy 0.3750 | ETputs(KTEPS) 206.00\n",
      "Epoch 00076 | Time(s) 0.0046 | Loss 0.5180 | Accuracy 0.3750 | ETputs(KTEPS) 206.05\n",
      "Epoch 00077 | Time(s) 0.0046 | Loss 0.4965 | Accuracy 0.3750 | ETputs(KTEPS) 205.94\n",
      "Epoch 00078 | Time(s) 0.0046 | Loss 0.4981 | Accuracy 0.3750 | ETputs(KTEPS) 206.05\n",
      "Epoch 00079 | Time(s) 0.0046 | Loss 0.5228 | Accuracy 0.3750 | ETputs(KTEPS) 205.57\n",
      "Epoch 00080 | Time(s) 0.0046 | Loss 0.5126 | Accuracy 0.3750 | ETputs(KTEPS) 204.78\n",
      "Epoch 00081 | Time(s) 0.0046 | Loss 0.4909 | Accuracy 0.3750 | ETputs(KTEPS) 204.64\n",
      "Epoch 00082 | Time(s) 0.0046 | Loss 0.4935 | Accuracy 0.3750 | ETputs(KTEPS) 204.24\n",
      "Epoch 00083 | Time(s) 0.0046 | Loss 0.4959 | Accuracy 0.3750 | ETputs(KTEPS) 204.46\n",
      "Epoch 00084 | Time(s) 0.0046 | Loss 0.4890 | Accuracy 0.3750 | ETputs(KTEPS) 204.09\n",
      "Epoch 00085 | Time(s) 0.0046 | Loss 0.5164 | Accuracy 0.3750 | ETputs(KTEPS) 204.20\n",
      "Epoch 00086 | Time(s) 0.0046 | Loss 0.4854 | Accuracy 0.3750 | ETputs(KTEPS) 204.41\n",
      "Epoch 00087 | Time(s) 0.0046 | Loss 0.4867 | Accuracy 0.3750 | ETputs(KTEPS) 204.60\n",
      "Epoch 00088 | Time(s) 0.0046 | Loss 0.5131 | Accuracy 0.3750 | ETputs(KTEPS) 204.85\n",
      "Epoch 00089 | Time(s) 0.0046 | Loss 0.4868 | Accuracy 0.3750 | ETputs(KTEPS) 204.83\n",
      "Epoch 00090 | Time(s) 0.0046 | Loss 0.5085 | Accuracy 0.3750 | ETputs(KTEPS) 205.25\n",
      "Epoch 00091 | Time(s) 0.0046 | Loss 0.5309 | Accuracy 0.3750 | ETputs(KTEPS) 205.54\n",
      "Epoch 00092 | Time(s) 0.0046 | Loss 0.4881 | Accuracy 0.3750 | ETputs(KTEPS) 205.79\n",
      "Epoch 00093 | Time(s) 0.0046 | Loss 0.4816 | Accuracy 0.3750 | ETputs(KTEPS) 206.10\n",
      "Epoch 00094 | Time(s) 0.0046 | Loss 0.4734 | Accuracy 0.3750 | ETputs(KTEPS) 206.10\n",
      "Epoch 00095 | Time(s) 0.0046 | Loss 0.4893 | Accuracy 0.3750 | ETputs(KTEPS) 206.46\n",
      "Epoch 00096 | Time(s) 0.0045 | Loss 0.4742 | Accuracy 0.3750 | ETputs(KTEPS) 206.76\n",
      "Epoch 00097 | Time(s) 0.0045 | Loss 0.5340 | Accuracy 0.3750 | ETputs(KTEPS) 206.97\n",
      "Epoch 00098 | Time(s) 0.0045 | Loss 0.4761 | Accuracy 0.3750 | ETputs(KTEPS) 207.23\n",
      "Epoch 00099 | Time(s) 0.0045 | Loss 0.5232 | Accuracy 0.3750 | ETputs(KTEPS) 207.48\n",
      "\n",
      "Test accuracy 37.50%\n"
     ]
    }
   ],
   "source": [
    "model = GCN(g,\n",
    "            in_feats,\n",
    "            args.n_hidden,\n",
    "            n_classes,\n",
    "            args.n_layers,\n",
    "            F.relu,\n",
    "            args.dropout)\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# use optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=args.lr,\n",
    "                             weight_decay=args.weight_decay)\n",
    "\n",
    "# initialize graph\n",
    "dur = []\n",
    "for epoch in range(args.n_epochs):\n",
    "    model.train()\n",
    "    if epoch >= 3:\n",
    "        t0 = time.time()\n",
    "    # forward\n",
    "    logits = model(features)\n",
    "    loss = loss_fcn(logits[train_mask], labels[train_mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch >= 3:\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "    acc = evaluate(model, features, labels, val_mask)\n",
    "    print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | Accuracy {:.4f} | \"\n",
    "          \"ETputs(KTEPS) {:.2f}\". format(epoch, np.mean(dur), loss.item(),\n",
    "                                         acc, n_edges / np.mean(dur) / 1000))\n",
    "\n",
    "print()\n",
    "acc = evaluate(model, features, labels, test_mask)\n",
    "print(\"Test accuracy {:.2%}\".format(acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
