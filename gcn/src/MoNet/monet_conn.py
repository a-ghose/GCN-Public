# -*- coding: utf-8 -*-
"""Monet_Connectome.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YZnUmhmtc7ykrs5ZAv1AixbmIbR-Iyp1
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount("/content/gdrive")
# %cd gdrive/My\ Drive/GCN_AD/GCN-17-master/lib3
!pip install dgl-cu100

import sys
sys.path.insert(0, '..')
from lib3 import utils
import argparse
import time
import numpy as np
import networkx as nx
import torch
import torch.nn as nn
import torch.nn.functional as F
import dgl
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from dgl.data import register_data_args, load_data
from dgl.nn.pytorch.conv import ChebConv, GMMConv
from dgl.nn.pytorch.glob import MaxPooling
from grid_graph import grid_graph
from coarsening import coarsen
from coordinate import get_coordinates, z2polar
args, unknown = utils.imp_arg()
y = utils.load_y()
aparcl,aparcc,aparc2l,aparc2c  = utils.load_connect()

X=aparc2c.transpose([2,0,1])
X_g = aparc2c.transpose([2,0,1])

import pandas as pd
from sklearn.model_selection import StratifiedKFold, train_test_split
import random
from sklearn.utils import shuffle
import scipy
import numpy as np
bin_ixs = []


# smi=1 , mci=2, ad=3

smiloc=np.asarray(np.where(y==1))
smiloc=np.ndarray.transpose(smiloc)
mciloc=np.asarray(np.where(y==2))
mciloc=np.ndarray.transpose(mciloc)
adloc=np.asarray(np.where(y==3))
adloc=np.ndarray.transpose(adloc)

#mci= 0 ad=1
#2v3, mci=0, ad=1
totalsize=len(mciloc)+len(adloc)
y_admci=np.zeros([totalsize])
X_admci=np.zeros([totalsize,164,164])
for i in range(len(mciloc)):
    y_admci[i]=0
    X_admci[i,:]=X[mciloc[i],:]
for j in range(len(adloc)):
    y_admci[len(mciloc)+j]=1
    X_admci[len(mciloc)+j,:]=X[adloc[j],:]
#print(y_admci.shape)
#print(X_admci.shape)



skf = StratifiedKFold(n_splits=args.fold,shuffle=True,random_state=23)
for train_index, test_index in skf.split(X_admci, y_admci):
    bin_ixs.append(test_index)


test_ixs = bin_ixs[-2:]
train_ixs = bin_ixs[:-2]
train_index = []
test_index = []
for b in list(range(len(train_ixs))):
  train_index = np.concatenate((train_index,train_ixs[b]))
for b in list(range(len(test_ixs))):
  test_index = np.concatenate((test_index,test_ixs[b]))
#print(train_index, len(train_index))
#print(y_admci)
#print(bin_ixs)
features = X_admci
labels = y_admci  
#print(len(train_index),len(test_index))


#print(len(test_ixs),len(train_ixs))
#print(labels)

train_index=train_index.astype('int')
test_index=test_index.astype('int')
#print(features[train_index].shape,labels[train_index].shape)

feats = []
for i in features:
  a = i.mean()
  feats.append(np.array([a]*164))
  
feats = np.array(feats)
feats.reshape(108,-1)
print("Node Feature Shape:",feats.shape)

X_train = feats[train_index]
y_train = labels[train_index]
X_test = feats[test_index]
y_test = labels[test_index]
from torch.utils.data import DataLoader, TensorDataset
from torch import Tensor


trainset = TensorDataset( Tensor(X_train), Tensor(y_train) )
testset = TensorDataset( Tensor(X_test), Tensor(y_test) )

##Get graphs for 108 patients in AD v MCI



mats = []
for i in X_admci:
  a = scipy.sparse.csr_matrix(i)
  mats = np.append(mats,a)
  
g_arr = [dgl.from_scipy(i,eweight_name="a") for i in mats]

print("Total Graph #:",len(g_arr))

coordinate_mat=pd.read_csv('../data/fs_a2009s_coordinate.csv',header=0,usecols=[3,4])

coordinate_mat = np.array(coordinate_mat)
#print(coordinate_mat.shape,type(coordinate_mat))
coordinate_mat = coordinate_mat.astype('float32')
coordinate_arr = []
for [a,b] in coordinate_mat:
  i = torch.tensor([a,b])
  coordinate_arr.append(i)
coordinate_arr = torch.stack(coordinate_arr)
coordinate_arr = [coordinate_arr]

print(len(coordinate_arr))
for g in g_arr:
  g.ndata['xy'] = coordinate_arr[0]
  g.apply_edges(z2polar)

print("GINFO:" ,g_arr[0].number_of_nodes)

def batcher(batch):
    x_batch = []
    y_batch = []
    for x, y in batch:
        x = x.view(-1)
        x_batch.append(x)
        y_batch.append(y)
        
    
    x_batch = torch.cat(x_batch).unsqueeze(-1)
    y_batch = torch.LongTensor(y_batch)
    return x_batch, y_batch

train_loader = DataLoader(trainset,
                          batch_size=args.batch_size,
                          shuffle=False,
                          collate_fn=batcher,
                          num_workers=0)
test_loader = DataLoader(testset,
                         batch_size=args.batch_size,
                         shuffle=False,
                         collate_fn=batcher,
                         num_workers=0)

class MoNet(nn.Module):
    def __init__(self,
                 n_kernels,
                 in_feats,
                 hiddens,
                 out_feats):
        super(MoNet, self).__init__()
        self.pool = nn.MaxPool1d(1)
        self.layers = nn.ModuleList()
        self.readout = MaxPooling()

        # Input layer
        self.layers.append(
            GMMConv(in_feats, hiddens[0], 1, n_kernels))

        # Hidden layer
        for i in range(1, len(hiddens)):
            self.layers.append(GMMConv(hiddens[i - 1], hiddens[i], 1, n_kernels))

        self.cls = nn.Sequential(
            nn.Linear(hiddens[-1], out_feats),
            nn.LogSoftmax()
        )

    def forward(self, g_arr, feat):
        for g, layer in zip(g_arr, self.layers):
            u = g.edata['a'].to(feat.device)
            #print(g.number_of_nodes)
            feat = self.pool(layer(g, feat, u).transpose(-1, -2).unsqueeze(0))\
                .squeeze(0).transpose(-1, -2)
        return self.cls(self.readout(g_arr[-1], feat))

class ChebNet(nn.Module):
    def __init__(self,
                 k,
                 in_feats,
                 hiddens,
                 out_feats):
        super(ChebNet, self).__init__()
        self.pool = nn.MaxPool1d(2)
        self.layers = nn.ModuleList()
        self.readout = MaxPooling()

        # Input layer
        self.layers.append(
            ChebConv(in_feats, hiddens[0], k))

        for i in range(1, len(hiddens)):
            self.layers.append(
                ChebConv(hiddens[i - 1], hiddens[i], k))

        self.cls = nn.Sequential(
            nn.Linear(hiddens[-1], out_feats),
            nn.LogSoftmax()
        )

    def forward(self, g_arr, feat):
        #for g, layer in zip(g_arr, self.layers):
            #feat = self.pool(layer(g, feat, [2] * g.batch_size).transpose(-1, -2).unsqueeze(0))\
               # .squeeze(0).transpose(-1, -2)
        return self.cls(self.readout(g_arr[-1], feat))

if args.gpu == -1:
    device = torch.device('cpu')
else:
    device = torch.device(args.gpu)

if args.model == 'chebnet':
    model = ChebNet(2, 1, [32, 64, 128, 256], 2)
else:
    model = MoNet(10, 1, [128, 128, 64, 128], 2)

model = model.to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
log_interval = 50

for epoch in range(30):
    print('epoch {} starts'.format(epoch))
    model.train()
    hit, tot = 0, 0
    loss_accum = 0
    for i, (x, y) in enumerate(train_loader):
       # print(g.number_of_nodes)
        #print("X shape:",x.shape)
        #print("y shape:",y.shape)
        x = x.to(device)
        #print(x.shape)
        y = y.to(device)
        g = [dgl.add_self_loop(g_arr[train_index[i]])]
        out = model(g, x)
        hit += (out.max(-1)[1] == y).sum().item()
        tot += len(y)
        loss = F.nll_loss(out, y)
        loss_accum += loss.item()

        if (i + 1) % log_interval == 0:
            print('loss: {}, acc: {}'.format(loss_accum / log_interval, hit / tot))
            hit, tot = 0, 0
            loss_accum = 0

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    model.eval()
    hit, tot = 0, 0
    for i, (x, y) in enumerate(test_loader):
        x = x.to(device)
        y = y.to(device)
        g = [dgl.add_self_loop(g_arr[test_index[i]])]
        out = model(g, x)
        hit += (out.max(-1)[1] == y).sum().item()
        tot += len(y)

    print('test acc: ', hit / tot)

