# -*- coding: utf-8 -*-
"""ADSMI_Monet_Connectome.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k3ExGGhJxzk63hCLq_HsZos-mbyeri2R
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount("/content/gdrive")
# %cd gdrive/My\ Drive/GCN_AD/GCN-17-master/lib3
!pip install dgl-cu100

import sys
sys.path.insert(0, '..')
from lib3 import utils
import argparse
import time
import numpy as np
import networkx as nx
import torch
import torch.nn as nn
import torch.nn.functional as F
import dgl
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from dgl.data import register_data_args, load_data
from dgl.nn.pytorch.conv import ChebConv, GMMConv
from dgl.nn.pytorch.glob import MaxPooling
from grid_graph import grid_graph
from coarsening import coarsen
from coordinate import get_coordinates, z2polar
args, unknown = utils.imp_arg()
y = utils.load_y()
aparcl,aparcc,aparc2l,aparc2c  = utils.load_connect()

###Features
###Feature matrix (179,164,164) is using length which is the average length of streamlines at each of the 164 Regions of Interest in the brain
X=aparc2l.transpose([2,0,1])

import pandas as pd
from sklearn.model_selection import StratifiedKFold, train_test_split
import random
from sklearn.utils import shuffle
import scipy
import numpy as np
bin_ixs = []


# smi=1 , mci=2, ad=3

smiloc=np.asarray(np.where(y==1))
smiloc=np.ndarray.transpose(smiloc)
mciloc=np.asarray(np.where(y==2))
mciloc=np.ndarray.transpose(mciloc)
adloc=np.asarray(np.where(y==3))
adloc=np.ndarray.transpose(adloc)

#mci= 0 ad=1
#2v3, mci=0, ad=1
totalsize=len(smiloc)+len(adloc)
y_adsmi=np.zeros([totalsize])
X_L=np.zeros([totalsize,164,164])
X_C = np.zeros([totalsize,164,164])
X_adsmi = np.zeros([totalsize,164,164])
for i in range(len(smiloc)):
    y_adsmi[i]=0
    X_adsmi[i,:]=X[smiloc[i],:]
for j in range(len(adloc)):
    y_adsmi[len(smiloc)+j]=1
    X_adsmi[len(smiloc)+j,:]=X[adloc[j],:]
    
#print(y_admci.shape)
#print(X_admci.shape)
test=scipy.io.loadmat('../data/IDP_morph_100.mat')
morphdata = np.array(test['M2_new'])
print(morphdata.shape)
X_adsmi, y_adsmi= shuffle(X_adsmi, y_adsmi, random_state=26)

skf = StratifiedKFold(n_splits=args.fold,shuffle=True,random_state=23)
for train_index, test_index in skf.split(X_adsmi, y_adsmi):
    bin_ixs.append(test_index)


test_ixs = bin_ixs[-2:]
train_ixs = bin_ixs[:-2]
train_index = []
test_index = []
for b in list(range(len(train_ixs))):
  train_index = np.append(train_index,train_ixs[b])
for b in list(range(len(test_ixs))):
  test_index = np.append(test_index,test_ixs[b])
#print(train_index, len(train_index))
#print(y_admci)
#print(bin_ixs)
features = X_adsmi
labels = y_adsmi  
#print(len(train_index),len(test_index))


#print(len(test_ixs),len(train_ixs))
#print(labels)
print("Connectome Length Shape:",X_adsmi.shape)

###Baseline Flattened Features
###Features flattened for baseline algorithm
Xbl = X_adsmi.reshape(119,-1)

###Setting index types so it runs
train_index=train_index.astype('int')
test_index=test_index.astype('int')
#print(features[train_index].shape,labels[train_index].shape)

### The mean of the 20 nearest neighbors of each node is taken to be applied as node features. (The node value was 0 before)
###Creating Node Feature Matrix
feats = []
for i in features:
  a = []
  for row in i:
    b = np.sort(row)
    a.append(b[:20].mean())
  feats.append(np.array(a))

feats = np.array(feats)

print("Node Feature Shape:",feats.shape)

###Creating Train and Test sets for node features
X_train = feats[train_index]
y_train = labels[train_index]
X_test = feats[test_index]
y_test = labels[test_index]
from torch.utils.data import DataLoader, TensorDataset
from torch import Tensor


trainset = TensorDataset( Tensor(X_train), Tensor(y_train) )
testset = TensorDataset( Tensor(X_test), Tensor(y_test) )

##Get graphs for 108 patients in AD v MCI
###Creating Graphs from the connectivity matrices of connectome length. (Edge data is maintained with "e_weight" parameter)
mats = []
for i in X_adsmi:
  a = scipy.sparse.csr_matrix(i)
  mats = np.append(mats,a)
  
g_arr = [dgl.from_scipy(i,eweight_name='a') for i in mats]

print("Total Graph #:",len(g_arr))

###Applying Coordinate of Each Region
coordinate_mat=pd.read_csv('../data/fs_a2009s_coordinate.csv',header=0,usecols=[3,4])
coordinate_mat = np.array(coordinate_mat)
#print(coordinate_mat.shape,type(coordinate_mat))
coordinate_mat = coordinate_mat.astype('float32')
coordinate_arr = []
for [a,b] in coordinate_mat:
  i = torch.tensor([a,b])
  coordinate_arr.append(i)
coordinate_arr = torch.stack(coordinate_arr)
coordinate_arr = [coordinate_arr]

print(len(coordinate_arr))
for g in g_arr:
  g.ndata['xy'] = coordinate_arr[0]
  g.apply_edges(z2polar)

###Defining batch function for data loader
def batcher(batch):
    x_batch = []
    y_batch = []
    for x, y in batch:
        x_batch.append(x)
        y_batch.append(y)
        
    
    x_batch = torch.cat(x_batch).unsqueeze(-1)
    y_batch = torch.LongTensor(y_batch)
    return x_batch, y_batch
###Loading Data
train_loader = DataLoader(trainset,
                          batch_size=args.batch_size,
                          shuffle=False,
                          collate_fn=batcher,
                          num_workers=0)
test_loader = DataLoader(testset,
                         batch_size=args.batch_size,
                         shuffle=False,
                         collate_fn=batcher,
                         num_workers=0)
###Monet Class Definition (Chebnet not used)
class MoNet(nn.Module):
    def __init__(self,
                 n_kernels,
                 in_feats,
                 hiddens,
                 out_feats):
        super(MoNet, self).__init__()
        self.pool = nn.MaxPool1d(1)
        self.layers = nn.ModuleList()
        self.readout = MaxPooling()

        # Input layer
        self.layers.append(
            GMMConv(in_feats, hiddens[0], 1, n_kernels))

        # Hidden layer
        for i in range(1, len(hiddens)):
            self.layers.append(GMMConv(hiddens[i - 1], hiddens[i], 1, n_kernels))

        self.cls = nn.Sequential(
            nn.Linear(hiddens[-1], out_feats),
            nn.LogSoftmax()
        )

    def forward(self, g_arr, feat):
        for g, layer in zip(g_arr, self.layers):
            u = g.edata['a'].to(feat.device)
            #print(g.number_of_nodes)
            feat = self.pool(layer(g, feat, u).transpose(-1, -2).unsqueeze(0))\
                .squeeze(0).transpose(-1, -2)
        return self.cls(self.readout(g_arr[-1], feat))

class ChebNet(nn.Module):
    def __init__(self,
                 k,
                 in_feats,
                 hiddens,
                 out_feats):
        super(ChebNet, self).__init__()
        self.pool = nn.MaxPool1d(2)
        self.layers = nn.ModuleList()
        self.readout = MaxPooling()

        # Input layer
        self.layers.append(
            ChebConv(in_feats, hiddens[0], k))

        for i in range(1, len(hiddens)):
            self.layers.append(
                ChebConv(hiddens[i - 1], hiddens[i], k))

        self.cls = nn.Sequential(
            nn.Linear(hiddens[-1], out_feats),
            nn.LogSoftmax()
        )

    def forward(self, g_arr, feat):
        #for g, layer in zip(g_arr, self.layers):
            #feat = self.pool(layer(g, feat, [2] * g.batch_size).transpose(-1, -2).unsqueeze(0))\
               # .squeeze(0).transpose(-1, -2)
        return self.cls(self.readout(g_arr[-1], feat))

if args.gpu == -1:
    device = torch.device('cpu')
else:
    device = torch.device(args.gpu)

if args.model == 'chebnet':
    model = ChebNet(2, 1, [32, 64, 128, 256], 2)
else:
    model = MoNet(5, 1, [100, 64, 128, 100], 2)

model = model.to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
log_interval = 50

for epoch in range(10):
    print('epoch {} starts'.format(epoch))
    model.train()
    hit, tot = 0, 0
    loss_accum = 0
    for i, (x, y) in enumerate(train_loader):
       # print(g.number_of_nodes)
        #print("X shape:",x.shape)
        #print("y shape:",y.shape)
        x = x.to(device)
        #print(x.shape)
        y = y.to(device)
        ###Add self loop to graph
        g = [dgl.add_self_loop(g_arr[train_index[i]])]
        out = model(g, x)
        hit += (out.max(-1)[1] == y).sum().item()
        tot += len(y)
        loss = F.nll_loss(out, y)
        loss_accum += loss.item()

        if (i + 1) % log_interval == 0:
            print('loss: {}, acc: {}'.format(loss_accum / log_interval, hit / tot))
            hit, tot = 0, 0
            loss_accum = 0

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    model.eval()
    hit, tot = 0, 0
    for i, (x, y) in enumerate(test_loader):
        x = x.to(device)
        y = y.to(device)
        g = [dgl.add_self_loop(g_arr[test_index[i]])]
        out = model(g, x)
        hit += (out.max(-1)[1] == y).sum().item()
        tot += len(y)

    print('test acc: ', hit / tot)

###Baseline Acc
from xgboost import XGBRFClassifier
features = features
labels = labels
xgb_model = XGBRFClassifier(random_state=42)
train_ix =[]
for i in range(len(train_ixs)):
  train_index = train_ixs[i]
  for n in train_index:
    train_ix.append(n)
    #print(n)

xgb_model.fit(Xbl[train_ix],labels[train_ix])
predictions = []
actual = []
correct = 0
for i in range(len(test_ixs)):
  test_index = test_ixs[i]
  preds = xgb_model.predict(Xbl[test_index])
  act = labels[test_index]
  predictions = np.append(predictions,preds)
  actual = np.append(actual,act)
#print(len(predictions))

for i in range(len(predictions)):
  if predictions[i] == actual[i]:
    correct +=1

bl_test_acc = correct/len(predictions)

print("Baseline Accuracy: {}".format(bl_test_acc))

